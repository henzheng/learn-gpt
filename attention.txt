# Three methods to measure similarity:
# Generally, greater value = more similar
# In extreme cases, a dot product or cosine similarity of 0 means no correlation

# Measure 1: Dot product
# Calculating the dot product of two matrices gives us a measure of similarity
# Where each number in the matrix may represent a different attribute of the word's meaning
# Dot product of 0 indicates no similarity (points are perpendicular)
# Can result in a negative value as well
 
# Measure 2: Cosine similarity
# Calculating the cosine of the angle between the two vectors from the origin
# Angle can be found with arccosine (using our vectors)
# A value of 1 would indicate the word being compared to itself
# Cos(0) = 1 (No distance between the points)

# Dot product and cosine similarity are alike 
# If vectors are scaled down to length 1 and placed on the unit circle, they are the same up to a scalar

# Measure 3: Scaled dot product
# Dot product divided by the square root of the length of the vector (number of coordinates or values in the matrix)
# Measure used in attention

# Attention: Adjust position for a word vector based on similarity
# New position is the sum of the word's dot product with each other word in the sentence (add all our similarities together)

# Normalization: similarity values should be normalized to smaller values for easier computation
# We want percentages where the coefficients add to 1.
# Achieve this by dividing by the sum of our coefficients.
# However, this does not account for negative values of similarity, which can result in dividing by 0.

# So we use Softmax: where the coefficient in our calculations is replaced by e^(coefficient)
# This method preserves the magnitude of similarity while making everything positive.
# If you simply made every negative value positive, it ruins the measure of similarity.
# However, e^0 is not the same as a coefficient of 0. In practice, however, it comes out as a negligible number. 
# We use our coefficients as a percent to move them that amount towards the word they were being compared to.

# Get new embeddings from existing ones by applying linear transformations. (Multiplying all values in a matrix by a scalar)
# Keys and Queries matrix helps us find the best embeddings (ones that relate words the best when applying attention)
# K and Q modify our embeddings with matrix multiplication, and then we can calculate similarity as usual
# Knows features of the words (color, size, features), more similar to transformers encoder

# Values matrix (more similar to transformers decoder)
# Multiplies our embedding with K and Q (optimized for finding similarities) by matrix V to create an embedding optimized for generation (finding the next word)
# Another linear transformation 
# Knows when two words can appear in the same context (e.g. apple or orange can both serve the same purpose)

# K, Q, and V come from weights trained with the transformer model.
# Feed forward neural networks attempt to compute the next word or token.